# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jez2ywrhpcd726tkW2YgODXTWgOyB8Og
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken

import os
os.environ['OPENAI_API_KEY'] = ''
print(os.getenv('OPENAI_API_KEY'))

from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings


class LLMRAGBasedNavigator:
    def __init__(self):
        self.__model = ChatOpenAI()
        self.__navigator_prompt_template = template = """
                    Provide the location of the product for the given customer's query in the Question
                    1. Classify the product requested in the customer's query under one of the following categories:
                        - Vegetables & Fruits
                        - Cereals
                        - Milk products
                        - Household cleaning
                        - Baby products
                        - Chocolates
                    2. Based on the category, follow the below location instructions and identify the location of the product:
                    {context}
                    3. Generate the answer in the format of json.

                    Question: {question}
                  """

    def __create_retreiver(self, context):
        self.__vectorstore = FAISS.from_texts(context, embedding=OpenAIEmbeddings())
        self.__retriever = self.__vectorstore.as_retriever()

    def __create_prompt(self):
        self.__prompt = ChatPromptTemplate.from_template(self.__navigator_prompt_template)

    def create_llm_chat_context(self, context):
        self.__create_retreiver(context)
        self.__create_prompt()
        self.__llm_chain = (
            {"context": self.__retriever, "question": RunnablePassthrough()}
            | self.__prompt
            | self.__model
            | StrOutputParser()
        )

    def chat_with_llm(self, question):
        response = self.__llm_chain.invoke(question)
        print(response)

context_for_store_navigation =  ["Vegetables & Fruits are in VF-Section A",
                                    "Cereals are in Cereals-Section B",
                                    "Milk products are in Milk products-Section C",
                                    "Household cleaning are in Household-Section D",
                                    "Baby products are in Baby-Section E",
                                    "Chocolates are in Sweets-Section F"]
llm_navigator = LLMRAGBasedNavigator()
llm_navigator.create_llm_chat_context(context_for_store_navigation)

llm_navigator.chat_with_llm("where is onion?")

llm_navigator.chat_with_llm("where could i find tomato?")

llm_navigator.chat_with_llm("where could i find yoghurt?")

llm_navigator.chat_with_llm("where could one find Harpic?")

llm_navigator.chat_with_llm("where could one find Ritter sports?")

llm_navigator.chat_with_llm("where could I find pacifier?")

llm_navigator.chat_with_llm("how could I locate MÃ¼sli?")

llm_navigator.chat_with_llm("how could I locate Kelloggs?")

llm_navigator.chat_with_llm("where could I find Bottle food?")